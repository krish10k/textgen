{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teju1001/textgen/blob/main/NLP_T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "input_ids = tokenizer.encode(\"WebNLG: Ritesh | fail | class </s>\", return_tensors=\"pt\")  # Batch size 1\n",
        "input_ids=input_ids.to(dev)\n",
        "outputs = model.generate(input_ids)\n",
        "tokenizer.decode(outputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "835c4b6e-7f1c-4922-ea19-5ec7dca2d532",
        "id": "uYBFftElVIOL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Ritesh is a failure of the class.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf1uUqSEKKgk"
      },
      "source": [
        "## Installing the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6ki1-m-5UUA",
        "outputId": "bc088415-43f2-46ee-c4e3-fda8107d195a"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 25.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 60.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 69.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 27.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CalKR6o4KP22"
      },
      "source": [
        "## Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB6rH4GenWpN"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers.optimization import  Adafactor \n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GZK-pXuKUw9"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aWMq3DOKl34"
      },
      "source": [
        "## Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CTSGvJQVChn"
      },
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "url = 'https://gitlab.com/shimorina/webnlg-dataset/-/archive/master/webnlg-dataset-master.zip?path=release_v3.0/en/train'\n",
        "urllib.request.urlretrieve(url, 'web.zip')\n",
        "with zipfile.ZipFile('web.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('web')\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "files = glob.glob(\"/content/web/webnlg-dataset-master-release_v3.0-en-train/release_v3.0/en/train/**/*.xml\", recursive=True)\n",
        "triple_re=re.compile('(\\d)triples')\n",
        "data_dct={}\n",
        "for file in files:\n",
        "    tree = ET.parse(file)\n",
        "    root = tree.getroot()\n",
        "    triples_num=int(triple_re.findall(file)[0])\n",
        "    for sub_root in root:\n",
        "        for ss_root in sub_root:\n",
        "            strutured_master=[]\n",
        "            unstructured=[]\n",
        "            for entry in ss_root:\n",
        "                unstructured.append(entry.text)\n",
        "                strutured=[triple.text for triple in entry]\n",
        "                strutured_master.extend(strutured)\n",
        "            unstructured=[i for i in unstructured if i.replace('\\n','').strip()!='' ]\n",
        "            strutured_master=strutured_master[-triples_num:]\n",
        "            strutured_master_str=(' && ').join(strutured_master)\n",
        "            data_dct[strutured_master_str]=unstructured\n",
        "mdata_dct={\"prefix\":[], \"input_text\":[], \"target_text\":[]}\n",
        "for st,unst in data_dct.items():\n",
        "    for i in unst:\n",
        "        mdata_dct['prefix'].append('webNLG')\n",
        "        mdata_dct['input_text'].append(st)\n",
        "        mdata_dct['target_text'].append(i)\n",
        "\n",
        "\n",
        "df=pd.DataFrame(mdata_dct)\n",
        "df.to_csv('webNLG2020_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF8VsYm5r5mP",
        "outputId": "f28f5d3a-f9d8-4ad3-9df6-f1f6012ee268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ2uifxUnVgk"
      },
      "source": [
        "train_df=pd.read_csv('webNLG2020_train.csv', index_col=[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_dPYbsVK5zO"
      },
      "source": [
        "Trimming off a few data points and so that a batch would not leave any remainder, hence some lines of codes can be avoided (Okay, this might be a hackish way of doing it )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WpyPzBKyXf1"
      },
      "source": [
        "train_df=train_df.iloc[  :35000,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgsIcgFgRCwN"
      },
      "source": [
        "train_df=train_df.sample(frac = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGfaigiqnZW5"
      },
      "source": [
        "batch_size=8\n",
        "num_of_batches=len(train_df)/batch_size\n",
        "num_of_epochs=4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQDfdl3mxYf_"
      },
      "source": [
        "num_of_batches=int(num_of_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kfjkXbiKeyU"
      },
      "source": [
        "Checking for the GPU availability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHkcKfKRr1BC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9018e3-d3bc-47b8-d927-04169cbf6660"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    dev = torch.device(\"cuda:0\") \n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    dev = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on the GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JaUpURdLAla"
      },
      "source": [
        "## Loading the pretrained model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVAXjd6wsOM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6be6bd-228f-4680-d56f-bd49fb8e8bf9"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base', return_dict=True)\n",
        "#moving the model to device(GPU/CPU)\n",
        "model.to(dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx0uCmTvLJFb"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4oqM6giLLg4"
      },
      "source": [
        "## Initializing the Adafactor optimizer with parameter values suggested for t5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KmOzTQj0E7L"
      },
      "source": [
        "optimizer = Adafactor(\n",
        "    model.parameters(),\n",
        "    lr=1e-3,\n",
        "    eps=(1e-30, 1e-3),\n",
        "    clip_threshold=1.0,\n",
        "    decay_rate=-0.8,\n",
        "    beta1=None,\n",
        "    weight_decay=0.0,\n",
        "    relative_step=False,\n",
        "    scale_parameter=False,\n",
        "    warmup_init=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ggfjCkdLcar"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXw1NKtS1YV3"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def progress(loss,value, max=100):\n",
        "    return HTML(\"\"\" Batch loss :{loss}\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(loss=loss,value=value, max=max))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i7zx4DmC_ap"
      },
      "source": [
        "num_of_epochs=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMe1hKshLgJn"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTvda_lWx2nC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "114dd893-7d31-43cf-8a10-dfd7bbbdf709"
      },
      "source": [
        "#Sets the module in training mode\n",
        "model.train()\n",
        "\n",
        "loss_per_10_steps=[]\n",
        "for epoch in range(1,num_of_epochs+1):\n",
        "  print('Running epoch: {}'.format(epoch))\n",
        "  \n",
        "  running_loss=0\n",
        "\n",
        "  out = display(progress(1, num_of_batches+1), display_id=True)\n",
        "  for i in range(num_of_batches):\n",
        "    inputbatch=[]\n",
        "    labelbatch=[]\n",
        "    new_df=train_df[i*batch_size:i*batch_size+batch_size]\n",
        "    for indx,row in new_df.iterrows():\n",
        "      input = 'WebNLG: '+row['input_text']+'</s>' \n",
        "      labels = row['target_text']+'</s>'   \n",
        "      inputbatch.append(input)\n",
        "      labelbatch.append(labels)\n",
        "    inputbatch=tokenizer.batch_encode_plus(inputbatch,padding=True,max_length=400,return_tensors='pt')[\"input_ids\"]\n",
        "    labelbatch=tokenizer.batch_encode_plus(labelbatch,padding=True,max_length=400,return_tensors=\"pt\") [\"input_ids\"]\n",
        "    inputbatch=inputbatch.to(dev)\n",
        "    labelbatch=labelbatch.to(dev)\n",
        "\n",
        "    # clear out the gradients of all Variables \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward propogation\n",
        "    outputs = model(input_ids=inputbatch, labels=labelbatch)\n",
        "    loss = outputs.loss\n",
        "    loss_num=loss.item()\n",
        "    logits = outputs.logits\n",
        "    running_loss+=loss_num\n",
        "    if i%10 ==0:      \n",
        "      loss_per_10_steps.append(loss_num)\n",
        "    out.update(progress(loss_num,i, num_of_batches+1))\n",
        "\n",
        "    # calculating the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    #updating the params\n",
        "    optimizer.step()\n",
        "    \n",
        "  running_loss=running_loss/int(num_of_batches)\n",
        "  print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running epoch: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " Batch loss :0.29172569513320923\n",
              "        <progress\n",
              "            value='4374'\n",
              "            max='4376',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            4374\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 , Running loss: 0.49904146816900796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwVEDrdZ545G"
      },
      "source": [
        "## Plotting the loss over time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7quhDpSxxTGL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b5c82d3a-724e-40ed-9345-caee2fe78b8e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "   \n",
        "steps = [i*100 for i in range(len(loss_per_10_steps))]\n",
        "  \n",
        "plt.plot(steps, loss_per_10_steps)\n",
        "plt.title('Loss over time')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv3VHD585lc9"
      },
      "source": [
        "## Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('/content/drive/MyDrive/files/')"
      ],
      "metadata": {
        "id": "1dOLhzeyYJ8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD6M4tb8l2Vs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "201424a8-21d9-4bed-cc65-abb9b967fc84"
      },
      "source": [
        "model.eval()\n",
        "text=\"WebNLG: sidharth | hometown | Delhi && sidharth | play |  football </s>\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\")  # Batch size 1\n",
        "input_ids=input_ids.to(dev)\n",
        "outputs = model.generate(input_ids)\n",
        "tokenizer.decode(outputs[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Sidharth plays football for the sidharth club which is based in Delhi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "input_ids = tokenizer.encode(\"WebNLG: Fish | water | eat </s>\", return_tensors=\"pt\")  # Batch size 1\n",
        "input_ids=input_ids.to(dev)\n",
        "outputs = model.generate(input_ids)\n",
        "tokenizer.decode(outputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EE67qzHxz4MO",
        "outputId": "a6867568-3a2a-43bb-dd35-015edd749204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Fish is a water dish that can be found in eat.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "input_ids1 = tokenizer.encode(\"WebNLG: Butterscotch | icecream  | better | Strawbery </s>\", return_tensors=\"pt\")\n",
        "input_ids2 = tokenizer.encode(\"WebNLG: Butterscotch | icecream | Strawbery | better </s>\", return_tensors=\"pt\") \n",
        "input_ids3 = tokenizer.encode(\"WebNLG: Butterscotch | better | Strawbery | icecream </s>\", return_tensors=\"pt\") \n",
        "\n",
        "input_ids1=input_ids1.to(dev)\n",
        "input_ids2=input_ids2.to(dev)\n",
        "input_ids3=input_ids3.to(dev)\n",
        "\n",
        "outputs1 = model.generate(input_ids1)\n",
        "outputs2 = model.generate(input_ids2)\n",
        "outputs3 = model.generate(input_ids3)\n",
        "\n",
        "print (\"Sentence 1:\",tokenizer.decode(outputs1[0]))\n",
        "print (\"Sentence 2:\",tokenizer.decode(outputs2[0]))\n",
        "print (\"Sentence 3:\",tokenizer.decode(outputs3[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e004f2-4403-44f6-c9c8-ae06ac7f46f3",
        "id": "XiwiKa6zQped"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: <pad> The ice cream butterscotch is better than Strawbery.</s>\n",
            "Sentence 2: <pad> The ice cream of butterscotch is ice cream and it is better than Stra\n",
            "Sentence 3: <pad> The ice cream butterscotch is better than Strawbery.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "input_ids = tokenizer.encode(\"WebNLG: Ritesh | fail | class </s>\", return_tensors=\"pt\")  # Batch size 1\n",
        "input_ids=input_ids.to(dev)\n",
        "outputs = model.generate(input_ids)\n",
        "tokenizer.decode(outputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "835c4b6e-7f1c-4922-ea19-5ec7dca2d532",
        "id": "Xqhss6I10oQu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Ritesh is a failure of the class.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "input_ids = tokenizer.encode(\"WebNLG: Ritesh | fail | class </s>\", return_tensors=\"pt\")  # Batch size 1\n",
        "input_ids=input_ids.to(dev)\n",
        "outputs = model.generate(input_ids)\n",
        "tokenizer.decode(outputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "083db62d-6321-40a1-e732-8ce60f508c22",
        "id": "lqQjIZfGVOy_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Ritesh is a failure of the class.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f93TfKNV1brj",
        "outputId": "33ccc382-bee4-408e-81e3-713bb960744a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1620, 18207,   517,    10,  2325,  1820,   455,  1820,   281,  1820,\n",
              "           161,     1]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/webNLG2020_train.csv')"
      ],
      "metadata": {
        "id": "1SNjL0CT2B23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5fRrVeXE2NH2",
        "outputId": "3714a6d4-466d-404d-81fb-a19051ad9d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  prefix                                         input_text  \\\n",
              "0           0  webNLG  AmeriGas | regionServed | United_States && Ame...   \n",
              "1           1  webNLG  AmeriGas | regionServed | United_States && Ame...   \n",
              "2           2  webNLG  AmeriGas | regionServed | United_States && Ame...   \n",
              "3           3  webNLG  AmeriGas | regionServed | United_States && Ame...   \n",
              "4           4  webNLG  AmeriGas | regionServed | United_States && Ame...   \n",
              "\n",
              "                                         target_text  \n",
              "0  AmeriGas (founded on January 01, 1959) is loca...  \n",
              "1  Serving all regions of the United States, Amer...  \n",
              "2  AmeriGas (founded on January 01, 1959) is loca...  \n",
              "3  AmeriGas, located in King of Prussia, Pennsylv...  \n",
              "4  Located in King of Prussia, Pennsylvania in th...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01300eb3-8d62-431e-a687-e84785204896\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>prefix</th>\n",
              "      <th>input_text</th>\n",
              "      <th>target_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>webNLG</td>\n",
              "      <td>AmeriGas | regionServed | United_States &amp;&amp; Ame...</td>\n",
              "      <td>AmeriGas (founded on January 01, 1959) is loca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>webNLG</td>\n",
              "      <td>AmeriGas | regionServed | United_States &amp;&amp; Ame...</td>\n",
              "      <td>Serving all regions of the United States, Amer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>webNLG</td>\n",
              "      <td>AmeriGas | regionServed | United_States &amp;&amp; Ame...</td>\n",
              "      <td>AmeriGas (founded on January 01, 1959) is loca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>webNLG</td>\n",
              "      <td>AmeriGas | regionServed | United_States &amp;&amp; Ame...</td>\n",
              "      <td>AmeriGas, located in King of Prussia, Pennsylv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>webNLG</td>\n",
              "      <td>AmeriGas | regionServed | United_States &amp;&amp; Ame...</td>\n",
              "      <td>Located in King of Prussia, Pennsylvania in th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01300eb3-8d62-431e-a687-e84785204896')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-01300eb3-8d62-431e-a687-e84785204896 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-01300eb3-8d62-431e-a687-e84785204896');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = df['input_text']\n",
        "o = df['target_text']\n",
        "print(\"Input :\",i[234],\"Output :\",o[234],sep=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKgi766_3FGW",
        "outputId": "a179fc43-085c-4b91-9d33-01ea35c76cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input :\n",
            "William_Anders | dateOfRetirement | \"1969-09-01\" && William_Anders | status | \"Retired\" && William_Anders | birthDate | \"1933-10-17\" && William_Anders | occupation | Fighter_pilot && William_Anders | birthPlace | British_Hong_Kong && William_Anders | mission | Apollo_8\n",
            "Output :\n",
            "William Anders was born in British Hong Kong on October 17th 1933. He served as a fighter pilot and became a crew member on Apollo 8 before he retired in 1969.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owrlOe0L62WK"
      },
      "source": [
        "Before testing the model further, lets learn how to serialize it and load from the path"
      ]
    }
  ]
}